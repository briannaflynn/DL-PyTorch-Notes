{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import *\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from fastai import datasets\n",
    "import pickle, gzip, math\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "\n",
    "import time\n",
    "\n",
    "def timeit(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        now = time.time()\n",
    "        retval = func(*args, **kwargs)\n",
    "        print('{} took {:.5f}s'.format(func.__name__, time.time() - now))\n",
    "        return retval\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# if not exist, download mnist dataset\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    path = \"/work2/05515/bflynn/frontera/data/mnist.pkl.gz\"\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s): return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "train_mean,train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "# NB: Use training, not validation mean for validation set\n",
    "# Why? If you normalize the two datasets with differently, they'll be on \n",
    "# totally different scales from one another - won't be able to tell \n",
    "# the differences between images in both sets \n",
    "x_valid = normalize(x_valid, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.view((28,28)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why the hell do inits matter?\n",
    "\n",
    "And why you need a good one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): x = a @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan), tensor(nan))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the heck happened? Activation explosion!\n",
    "Let's ask the loop to break when our activations explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    x = a @ x\n",
    "    if x.std() != x.std(): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only takes 28 iterations until our activations explode!\n",
    "\n",
    "What about if we initialize our activations with a scale that is really low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): x = a @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now every activation has vanished, to zero. **This is the reason why we couldn't train deep neural networks for so long - initializing them is challenging and you need to pick the right initialization or your activations get too big to handle (nan) or vanish entirely!**\n",
    "\n",
    "Some strategies to properly initialize - \n",
    "\n",
    "- use stdev that will make sure x and ax have the *same* scale\n",
    "- orthogonal matrix initializes weight, so x and ax would have same sum of squares by preserving the L2 norm\n",
    "- spectral normalization on matrix A (spectral norm is least possible number M such that ```torch.norm(A@x) <= M*torch.norm(x)``` so dividing A by this M insures you don't overflow. You can still vanish with this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier initialization\n",
    "\n",
    "TLDR: scale equal to ```1/math.sqrt(n_in)```, where ```n_in``` is the **number of inputs of our matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "x = torch.randn(512)\n",
    "a = torch.randn(512,512)/math.sqrt(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): x = a @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1363), tensor(7.0877))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaiming initialization\n",
    "\n",
    "PyTorch kaiming init for reference ( just an example )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 32 # number of hidden layers just an example\n",
    "l1 = nn.Conv2d(1, nh, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.kaiming_normal_(l1.weight, a=1.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All you need is a good init - paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layering matrix multiplication, relu, fully connect it forward and then backward to pass back the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Data (vars initialized above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a loss function, without it we can't train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss\n",
    "\n",
    "First, we will need to compute the softmax of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in code form: \n",
    "\n",
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you multiply by zero in a one hot encoded vector, doing nothing and also slow\n",
    "# faster way - what is the location of what you care about? which is one\n",
    "# Use the index of our actual! (the one in a one hot encoded matrix or vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred.shape # sm = softmax predictions\n",
    "# 50,000 for all images in training set, 10 for number of pred categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.9466, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9466, -2.2533, -2.4473], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0,1,2], [5,0,4]] \n",
    "# works because pytorch does numpy advanced indexing!\n",
    "# integer array indexing\n",
    "\n",
    "# can pass a list for each dimension(2 dims, 2 lists)\n",
    "# first is the row indexes you want\n",
    "# second is the column indexes you want\n",
    "\n",
    "# returns 0,5 - 1,0 - 2,4\n",
    "# check that above and below 0,5 are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative log likelihood function\n",
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "# all row indexes, and targets is columns we want\n",
    "# include \"-\" before input - because negative log likelihood\n",
    "# then, take the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss is the negative log likelihood of softmax predictions, compared \n",
    "# to actual training (y_training)\n",
    "\n",
    "loss = nll(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3448, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the formula \n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax, which was previously defined as `(x.exp()/(x.exp().sum(-1,keepdim=True))).log()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.exp().sum(-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you go x.exp() - can get very big numbers\n",
    "# huge numbers in floating numbers on a computer, wildly inprecise\n",
    "# computer could thing two huge numbers 1000 apart are exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:, None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Training\n",
    "\n",
    "Basically the training loop repeats over the following steps:\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb): return (torch.argmax(out, dim=1) == yb).float().mean()\n",
    "# grab argmax, which number in softmax is highest, index of that is our prediction\n",
    "# (index would be what category was predicted in the one hot encoded matrix)\n",
    "# check whether that's equal to the actual\n",
    "# take the mean, convert it to a float first (can't take mean of int in pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1760, -0.3334,  0.6334, -0.0093, -0.2983,  0.5190, -0.2911,  0.5797,\n",
       "         -0.0902,  0.1106], grad_fn=<SelectBackward>),\n",
       " torch.Size([64, 10]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3418, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0312)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb) \n",
    "# haven't trained our model yet, so the accuracy based on the \n",
    "# randomly initialized parameters is 10 ish percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5 # learning rate\n",
    "epochs = 1 # how many epochs to train for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "- calculate predictions\n",
    "- calculate your loss\n",
    "- do backward pass\n",
    "- **subtract learning rate times gradients**\n",
    "- and then **zero the gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i : end_i]\n",
    "        yb = y_train[start_i : end_i]\n",
    "        loss = loss_func(model(xb), yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad*lr\n",
    "                    l.bias -= l.bias.grad*lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias  .grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2007, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using parameters and optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i+bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_func(model(xb), yb)\n",
    "\n",
    "            loss.backward()\n",
    "            ## with torch.no_grad() refactoring, replace with\n",
    "            # model.parameters\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2007, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModule():\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        # every time set an attribute, update a list of _modules wiht list of all modules I have\n",
    "    def __setattr__(self,k,v):\n",
    "        if not k.startswith(\"_\"): self._modules[k] = v\n",
    "        super().__setattr__(k,v)\n",
    "        \n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            for p in l.parameters(): yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = DummyModule(m,nh,10)\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__() ## it has to set up dummy modules _modules dictionary\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1:Linear(in_features=784, out_features=50, bias=True)\n",
      "l2:Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, l in model.named_children(): print(f\"{name}:{l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__() # needed to set up module dictionary\n",
    "        self.layers = layers\n",
    "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.ModuleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialModel(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optim\n",
    "\n",
    "**Further refactoring**\n",
    "\n",
    "Let's replace our previous manually coded optimization step:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "\n",
    "and instead use just:\n",
    "\n",
    "```python\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): \n",
    "        self.params,self.lr=list(params),lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * self.lr\n",
    "                \n",
    "    # select only parameters that you want to optimize, and zero \n",
    "    # their gradients\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8076, grad_fn=<NllLossBackward>), tensor(0.8125))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3353, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9692, grad_fn=<NllLossBackward>), tensor(0.7500))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When you're developing ...\n",
    "\n",
    "Don't set a random seed - if there's variation going on in the model at different times, we want to see it. \n",
    "\n",
    "#### Don't want it to be shielded by setting one random seed that works well\n",
    "\n",
    "Though reproducibility is key, great for doing science, not how you should develop your models at the beginning. Developing your models, you want intuitive sense of:\n",
    "- is it stable?\n",
    "- how much variation do you expect?\n",
    "\n",
    "If you have a test where it fails every 100 times, you want to know that! \n",
    "\n",
    "Can help with verifying stability and working to improve stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader\n",
    "\n",
    "It's clunky to iterate through minibatches of x and y values separately:\n",
    "\n",
    "```python\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducing a `Dataset` class:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x) # len of something - length of dataset now stored\n",
    "    def __getitem__(self, i): return self.x[i],self.y[i] # return tuple of x[i] and y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "assert len(train_ds)==len(x_train)\n",
    "assert len(valid_ds)==len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4244, -0.4244, -0.4244,  ..., -0.4244, -0.4244, -0.4244],\n",
       "         [-0.4244, -0.4244, -0.4244,  ..., -0.4244, -0.4244, -0.4244],\n",
       "         [-0.4244, -0.4244, -0.4244,  ..., -0.4244, -0.4244, -0.4244],\n",
       "         [-0.4244, -0.4244, -0.4244,  ..., -0.4244, -0.4244, -0.4244],\n",
       "         [-0.4244, -0.4244, -0.4244,  ..., -0.4244, -0.4244, -0.4244]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = train_ds[0:5]\n",
    "assert xb.shape==(5,28*28)\n",
    "assert yb.shape==(5,)\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs + 1):\n",
    "        xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8750)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self,ds, bs): self.ds, self.bs = ds,bs\n",
    "    # class that takes dataset and batch size, stores them away\n",
    "    # loop through range from 0 to size of datase\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): # up to 500000\n",
    "            yield self.ds[i:i+self.bs] # end at 1 + self. batch size\n",
    "            \n",
    "    # yield - co-routine \n",
    "    # have a function that doesn't return one thing once, \n",
    "    # return lots of things lots of times - each time you call next, return the next thing that is yielded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "assert xb.shape == (bs, 28*28)\n",
    "assert yb.shape == (bs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs): # go through each epoch\n",
    "        for xb, yb in train_dl: # go through each batch, grabbing independent and dependent variables\n",
    "            pred = model(xb) # calculate the predictions\n",
    "            loss = loss_func(pred, yb) # calculate the losses\n",
    "            loss.backward() # calculate the gradients\n",
    "            opt.step() # update with the learning rate\n",
    "            opt.zero_grad() # reset the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want code to be this good as a researcher, because it gives you the freedom to try new cool stuff without bugs that you didn't know about, without interpretability by your domain expert colleagues, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2686, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc > 0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is still problematic because it's going through the training set in the same order every time\n",
    "\n",
    "We want the randomness of order for better generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self): # grab random permutation if not already shuffled\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = Dataset(*train_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Sampler(small_ds, 3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Sampler(small_ds, 3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([4, 2, 7]), tensor([5, 6, 1]), tensor([0, 9, 3]), tensor([8])]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)\n",
    "# could change this if you want to add padding, etc\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "    # pass dataloader sampler, looping through with yield avoids memory issues\n",
    "    # grab indexes and ds at index, gives list of tensors, collate them into a single pair of tensors\n",
    "    # collate grabs x and y and glues them together on a new axis\n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2532, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch's dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2060, grad_fn=<NllLossBackward>), tensor(0.9688))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PyTorch's `DataLoader`, if you pass `num_workers`, will use multiple threads to call your `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid\n",
    "\n",
    "You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # Handle batchnorm / dropout\n",
    "        model.train()\n",
    "#         print(model.training)\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "#         print(model.training)\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                # no backwards - not calculating gradients for the \n",
    "                # validation set\n",
    "                tot_loss += loss_func(pred, yb) # keep track of loss\n",
    "                tot_acc  += accuracy (pred,yb) # keep track of acc\n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*: Are these validation results correct if batch size varies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You need to actually weight according to the size of the minibatch\n",
    "if you get the accuracy and loss for minibatch of 1 vs minibatch of 10001, can't really compare the two\n",
    "\n",
    "However, this is how almost every library does it\n",
    "\n",
    "Does NOT work correctly when you batch size varies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question - why do you have to zero out your gradients in pytorch?\n",
    "\n",
    "If we didn't zero out, when we go loss.backwards will add the new gradients to the existing gradients\n",
    "\n",
    "We often have lots of sources of gradients, lots of modules all connected together\n",
    "\n",
    "When we call backward we wouldn't want backward to zero the gradients\n",
    "\n",
    "We need the grad.zero after calling backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2141) tensor(0.9415)\n",
      "1 tensor(0.1781) tensor(0.9507)\n",
      "2 tensor(0.1783) tensor(0.9537)\n",
      "3 tensor(0.5228) tensor(0.8956)\n",
      "4 tensor(0.1647) tensor(0.9539)\n"
     ]
    }
   ],
   "source": [
    "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model,opt = get_model()\n",
    "loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks \n",
    "\n",
    "Callbacks let you fully customize every one of the steps\n",
    "- Model\n",
    "- Loss\n",
    "- Gradients\n",
    "- Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "nh,bs = 50,64\n",
    "c = y_train.max().item()+1\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor out the connected pieces of info out of the fit() argument list\n",
    "\n",
    "`fit(epochs, model, loss_func, opt, train_dl, valid_dl)`\n",
    "\n",
    "Let's replace it with something that looks like this:\n",
    "\n",
    "`fit(1, learn)`\n",
    "\n",
    "This will allow us to tweak what's happening inside the training loop in other places of the code because the `Learner` object will be mutable, so changing any of its attribute elsewhere will be seen in our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, c=None):\n",
    "        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "        \n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(data, lr=0.5, nh=50):\n",
    "    m = data.train_ds.x.shape[1]\n",
    "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(*get_model(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner has no logic at all - just a storage device for model, optimizer, loss function and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact same thing, except now we access our tools we need from \n",
    "# learn where we stored all our stuff in previously\n",
    "\n",
    "def fit(epochs, learn):\n",
    "    for epoch in range(epochs):\n",
    "        learn.model.train()\n",
    "        for xb,yb in learn.data.train_dl:\n",
    "            loss = learn.loss_func(learn.model(xb), yb)\n",
    "            loss.backward()\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "\n",
    "        learn.model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in learn.data.valid_dl:\n",
    "                pred = learn.model(xb)\n",
    "                tot_loss += learn.loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        nv = len(learn.data.valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3427) tensor(0.8992)\n"
     ]
    }
   ],
   "source": [
    "loss,acc = fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_batch(xb, yb, cb):\n",
    "    if not cb.begin_batch(xb,yb): return\n",
    "    loss = cb.learn.loss_func(cb.learn.model(xb), yb)\n",
    "    if not cb.after_loss(loss): return\n",
    "    loss.backward()\n",
    "    if cb.after_backward(): cb.learn.opt.step()\n",
    "    if cb.after_step(): cb.learn.opt.zero_grad()\n",
    "\n",
    "def all_batches(dl, cb):\n",
    "    for xb,yb in dl:\n",
    "        one_batch(xb, yb, cb)\n",
    "        if cb.do_stop(): return\n",
    "\n",
    "def fit(epochs, learn, cb):\n",
    "    if not cb.begin_fit(learn): return\n",
    "    for epoch in range(epochs):\n",
    "        if not cb.begin_epoch(epoch): continue\n",
    "        all_batches(learn.data.train_dl, cb)\n",
    "        \n",
    "        if cb.begin_validate():\n",
    "            with torch.no_grad(): all_batches(learn.data.valid_dl, cb)\n",
    "        if cb.do_stop() or not cb.after_epoch(): break\n",
    "    cb.after_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    def begin_fit(self, learn):\n",
    "        self.learn = learn\n",
    "        return True\n",
    "    def after_fit(self): return True\n",
    "    def begin_epoch(self, epoch):\n",
    "        self.epoch=epoch\n",
    "        return True\n",
    "    def begin_validate(self): return True\n",
    "    def after_epoch(self): return True\n",
    "    def begin_batch(self, xb, yb):\n",
    "        self.xb,self.yb = xb,yb\n",
    "        return True\n",
    "    def after_loss(self, loss):\n",
    "        self.loss = loss\n",
    "        return True\n",
    "    def after_backward(self): return True\n",
    "    def after_step(self): return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbackHandler():\n",
    "    def __init__(self,cbs=None):\n",
    "        self.cbs = cbs if cbs else []\n",
    "\n",
    "    def begin_fit(self, learn):\n",
    "        self.learn,self.in_train = learn,True\n",
    "        learn.stop = False\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_fit(learn)\n",
    "        return res\n",
    "\n",
    "    def after_fit(self):\n",
    "        res = not self.in_train\n",
    "        for cb in self.cbs: res = res and cb.after_fit()\n",
    "        return res\n",
    "    \n",
    "    def begin_epoch(self, epoch):\n",
    "        self.learn.model.train()\n",
    "        self.in_train=True\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_epoch(epoch)\n",
    "        return res\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.learn.model.eval()\n",
    "        self.in_train=False\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_validate()\n",
    "        return res\n",
    "\n",
    "    def after_epoch(self):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.after_epoch()\n",
    "        return res\n",
    "    \n",
    "    def begin_batch(self, xb, yb):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_batch(xb, yb)\n",
    "        return res\n",
    "\n",
    "    def after_loss(self, loss):\n",
    "        res = self.in_train\n",
    "        for cb in self.cbs: res = res and cb.after_loss(loss)\n",
    "        return res\n",
    "\n",
    "    def after_backward(self):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.after_backward()\n",
    "        return res\n",
    "\n",
    "    def after_step(self):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.after_step()\n",
    "        return res\n",
    "    \n",
    "    def do_stop(self):\n",
    "        try:     return self.learn.stop\n",
    "        finally: self.learn.stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    def begin_fit(self,learn):\n",
    "        super().begin_fit(learn)\n",
    "        self.n_iters = 0\n",
    "        return True\n",
    "        \n",
    "    def after_step(self):\n",
    "        self.n_iters += 1\n",
    "        print(self.n_iters)\n",
    "        if self.n_iters>=10: self.learn.stop = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "fit(1, learn, cb = CallbackHandler([TestCallback()])) # batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        cbs = listify(cbs)\n",
    "        for cbf in listify(cb_funcs):\n",
    "            cb = cbf()\n",
    "            setattr(self, cb.name, cb)\n",
    "            cbs.append(cb)\n",
    "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
    "\n",
    "    @property\n",
    "    def opt(self):       return self.learn.opt\n",
    "    @property\n",
    "    def model(self):     return self.learn.model\n",
    "    @property\n",
    "    def loss_func(self): return self.learn.loss_func\n",
    "    @property\n",
    "    def data(self):      return self.learn.data\n",
    "\n",
    "    def one_batch(self, xb, yb):\n",
    "        self.xb,self.yb = xb,yb\n",
    "        if self('begin_batch'): return\n",
    "        self.pred = self.model(self.xb)\n",
    "        if self('after_pred'): return\n",
    "        self.loss = self.loss_func(self.pred, self.yb)\n",
    "        if self('after_loss') or not self.in_train: return\n",
    "        self.loss.backward()\n",
    "        if self('after_backward'): return\n",
    "        self.opt.step()\n",
    "        if self('after_step'): return\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "    def all_batches(self, dl):\n",
    "        self.iters = len(dl)\n",
    "        for xb,yb in dl:\n",
    "            if self.stop: break\n",
    "            self.one_batch(xb, yb)\n",
    "            self('after_batch')\n",
    "        self.stop=False\n",
    "\n",
    "    def fit(self, epochs, learn):\n",
    "        self.epochs,self.learn = epochs,learn\n",
    "\n",
    "        try:\n",
    "            for cb in self.cbs: cb.set_runner(self)\n",
    "            if self('begin_fit'): return\n",
    "            for epoch in range(epochs):\n",
    "                self.epoch = epoch\n",
    "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
    "\n",
    "                with torch.no_grad(): \n",
    "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
    "                if self('after_epoch'): break\n",
    "            \n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.learn = None\n",
    "\n",
    "    def __call__(self, cb_name):\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "            f = getattr(cb, cb_name, None)\n",
    "            if f and f(): return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealing\n",
    "\n",
    "We define two new callbacks: the Recorder to save track of the loss and our scheduled learning rate, and a ParamScheduler that can schedule any hyperparameter as long as it's registered in the state_dict of the optimizer. \n",
    "\n",
    "Hyperparameter scheduling - learning rate\n",
    "\n",
    "You can and should schedule everything - dropout amount, weight decay, learning rate, momentum, everything\n",
    "\n",
    "Different phases in the loss landscapes of neural nets - look very different at the start, middle, and end \n",
    "\n",
    "Very unlikely that you want the same things for all parts of training so being able to schedule things is a good idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Recorder(Callback): # at the start of fitting, sets losses and lrs to empty\n",
    "    def begin_fit(self): self.lrs,self.losses = [],[]\n",
    "\n",
    "        # when training append the current lr and loss\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.lrs.append(self.opt.param_groups[-1]['lr']) # this is the final layer group learning rate\n",
    "        self.losses.append(self.loss.detach().cpu())        \n",
    "\n",
    "    def plot_lr  (self): plt.plot(self.lrs)\n",
    "    def plot_loss(self): plt.plot(self.losses)\n",
    "\n",
    "class ParamScheduler(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func\n",
    "\n",
    "    def set_param(self):\n",
    "        for pg in self.opt.param_groups: # layer groups = param groups; pytorch optimizer contains param_groups, loop through\n",
    "            pg[self.pname] = self.sched_func(self.n_epochs/self.epochs)\n",
    "    \n",
    "    # this says everytime we start a batch, run our scheduler\n",
    "    def begin_batch(self): \n",
    "        if self.in_train: self.set_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sched_lin(start, end):\n",
    "    def _inner(start, end, pos): return start + pos * (end-start)\n",
    "    return partial(_inner, start, end)\n",
    "# the function takes a start end and position of learning rate, returns a learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor this with a decorator\n",
    "\n",
    "def annealer(f):\n",
    "    def _inner(start, end): return partial(f, start, end)\n",
    "    return _inner\n",
    "\n",
    "@annealer\n",
    "def sched_lin(start, end, pos): return start + pos * (end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decorator is a function that returns a function\n",
    "Takes the written function, passes it into the function called with the '@' sign, and replaces the definition of the function with whatever is returned"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sched_lin() # use shift - tab; see that it takes only start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = sched_lin(1,2)\n",
    "f(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other scheduler functions\n",
    "@annealer\n",
    "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
    "@annealer\n",
    "def sched_no(start, end, pos):  return start\n",
    "@annealer\n",
    "def sched_exp(start, end, pos): return start * (end/start) ** pos\n",
    "\n",
    "def cos_1cycle_anneal(start, high, end):\n",
    "    return [sched_cos(start, high), sched_cos(high, end)]\n",
    "\n",
    "#This monkey-patch is there to be able to plot tensors\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "annealings = \"NO LINEAR COS EXP\".split()\n",
    "\n",
    "a = torch.arange(0, 100)\n",
    "p = torch.linspace(0.01,1,100)\n",
    "\n",
    "fns = [sched_no, sched_lin, sched_cos, sched_exp]\n",
    "for fn, t in zip(fns, annealings):\n",
    "    f = fn(2, 1e-2)\n",
    "    plt.plot(a, [f(o) for o in p], label=t)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine different schedulers (might want a warm up and cool down type scheduler)\n",
    "def combine_scheds(pcts, scheds):\n",
    "    assert sum(pcts) == 1.\n",
    "    pcts = tensor([0] + listify(pcts))\n",
    "    assert torch.all(pcts >= 0)\n",
    "    pcts = torch.cumsum(pcts, 0)\n",
    "    def _inner(pos):\n",
    "        idx = (pos >= pcts).nonzero().max()\n",
    "        if idx == 2: idx = 1\n",
    "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
    "        return scheds[idx](actual_pos)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = combine_scheds([0.3,0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b59d3907510>]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(a, [sched(o) for o in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to train at a high learning rate for a long time, but you also want to train at a very low learning rate for a long time, and you want a gentle learning rate at the beginning when things are fragile.\n",
    "\n",
    "Therefore, want cosine scheduler to warm up and cool down our learning rates as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgStats():\n",
    "    def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train\n",
    "    \n",
    "    def reset(self):\n",
    "        self.tot_loss,self.count = 0.,0\n",
    "        self.tot_mets = [0.] * len(self.metrics)\n",
    "        \n",
    "    @property\n",
    "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
    "    @property\n",
    "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if not self.count: return \"\"\n",
    "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
    "\n",
    "    def accumulate(self, run):\n",
    "        bn = run.xb.shape[0]\n",
    "        self.tot_loss += run.loss * bn\n",
    "        self.count += bn\n",
    "        for i,m in enumerate(self.metrics):\n",
    "            self.tot_mets[i] += m(run.pred, run.yb) * bn\n",
    "\n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        \n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.run)\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        print(self.train_stats)\n",
    "        print(self.valid_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs = [Recorder,\n",
    "        partial(AvgStatsCallback,accuracy),\n",
    "        partial(ParamScheduler, 'lr', sched)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learner(model_func, loss_func, data):\n",
    "    return Learner(*model_func(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_func(lr=0.5): return partial(get_model, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_learner(get_model, loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    _order=0\n",
    "    def set_runner(self, run): self.run=run\n",
    "    def __getattr__(self, k): return getattr(self.run, k)\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        return camel2snake(name or 'callback')\n",
    "    \n",
    "    def __call__(self, cb_name): # takes any callback that a user might make\n",
    "        f = getattr(self, cb_name, None)\n",
    "        if f and f(): return True # users can replace __call__ itself\n",
    "        return False # makes this more flexible\n",
    "\n",
    "class TrainEvalCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.run.n_epochs=0.\n",
    "        self.run.n_iter=0\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.run.n_epochs += 1./self.iters\n",
    "        self.run.n_iter   += 1\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.run.n_epochs=self.epoch\n",
    "        self.model.train()\n",
    "        self.run.in_train=True\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.run.in_train=False\n",
    "\n",
    "class CancelTrainException(Exception): pass # let peoples callbacks cancel at any of these levels \n",
    "class CancelEpochException(Exception): pass # example, can cancel one epoch but continue to the next one\n",
    "class CancelBatchException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        self.in_train = False\n",
    "        cbs = listify(cbs)\n",
    "        for cbf in listify(cb_funcs):\n",
    "            cb = cbf()\n",
    "            setattr(self, cb.name, cb)\n",
    "            cbs.append(cb)\n",
    "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
    "\n",
    "    @property\n",
    "    def opt(self):       return self.learn.opt\n",
    "    @property\n",
    "    def model(self):     return self.learn.model\n",
    "    @property\n",
    "    def loss_func(self): return self.learn.loss_func\n",
    "    @property\n",
    "    def data(self):      return self.learn.data\n",
    "\n",
    "    def one_batch(self, xb, yb):\n",
    "        try:\n",
    "            self.xb,self.yb = xb,yb\n",
    "            self('begin_batch')\n",
    "            self.pred = self.model(self.xb)\n",
    "            self('after_pred')\n",
    "            self.loss = self.loss_func(self.pred, self.yb)\n",
    "            self('after_loss')\n",
    "            if not self.in_train: return\n",
    "            self.loss.backward()\n",
    "            self('after_backward')\n",
    "            self.opt.step()\n",
    "            self('after_step')\n",
    "            self.opt.zero_grad()\n",
    "        except CancelBatchException: self('after_cancel_batch')\n",
    "        finally: self('after_batch')\n",
    "\n",
    "    def all_batches(self, dl):\n",
    "        self.iters = len(dl)\n",
    "        try:\n",
    "            for xb,yb in dl: self.one_batch(xb, yb)\n",
    "        except CancelEpochException: self('after_cancel_epoch')\n",
    "\n",
    "    def fit(self, epochs, learn):\n",
    "        self.epochs,self.learn,self.loss = epochs,learn,tensor(0.)\n",
    "\n",
    "        try:\n",
    "            for cb in self.cbs: cb.set_runner(self)\n",
    "            self('begin_fit')\n",
    "            for epoch in range(epochs):\n",
    "                self.epoch = epoch\n",
    "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
    "\n",
    "                with torch.no_grad(): \n",
    "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
    "                self('after_epoch')\n",
    "            \n",
    "        except CancelTrainException: self('after_cancel_train')\n",
    "        finally:\n",
    "            self('after_fit') \n",
    "            self.learn = None\n",
    "\n",
    "    def __call__(self, cb_name):\n",
    "        res = False\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on use of Exceptions\n",
    "\n",
    "Exceptions in this usage allow the callback writer to stop any of the main parts from things happening.\n",
    "\n",
    "In this example, will use it to stop training in order to create a learning rate finder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    _order=1\n",
    "    def after_step(self):\n",
    "        print(self.n_iter)\n",
    "        if self.n_iter>=10: raise CancelTrainException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Runner(cb_funcs=TestCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some other callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        \n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.run)\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        print(self.train_stats)\n",
    "        print(self.valid_stats)\n",
    "        \n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.lrs = [[] for _ in self.opt.param_groups]\n",
    "        self.losses = []\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())        \n",
    "\n",
    "    def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])\n",
    "    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])\n",
    "        \n",
    "    def plot(self, skip_last=0, pgid=-1):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        lrs    = self.lrs[pgid]\n",
    "        n = len(losses)-skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(lrs[:n], losses[:n])\n",
    "\n",
    "class ParamScheduler(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs\n",
    "        \n",
    "    def begin_fit(self):\n",
    "        if not isinstance(self.sched_funcs, (list,tuple)):\n",
    "            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n",
    "\n",
    "    def set_param(self):\n",
    "        assert len(self.opt.param_groups)==len(self.sched_funcs)\n",
    "        for pg,f in zip(self.opt.param_groups,self.sched_funcs):\n",
    "            pg[self.pname] = f(self.n_epochs/self.epochs)\n",
    "            \n",
    "    def begin_batch(self): \n",
    "        if self.in_train: self.set_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Learning Rate Finder from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to also add something that saves the model before running this, and loads it back after running - **otherwise you'll lose your weights!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_Find(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
    "        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n",
    "        self.best_loss = 1e9\n",
    "    # in begin batch, uses exponential curve to set the learning rate    \n",
    "    def begin_batch(self): \n",
    "        if not self.in_train: return\n",
    "        pos = self.n_iter/self.max_iter\n",
    "        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos # curve to set lr\n",
    "        for pg in self.opt.param_groups: pg['lr'] = lr\n",
    "    # after each step, checks to see if we've done more than the maximum number of iterations (default is 100)     \n",
    "    def after_step(self): # or, whether the loss is much worse than the best we've had \n",
    "        # if EITHER happens, we raise CancelTrainException, which cancels training\n",
    "        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n",
    "            raise CancelTrainException() # this cancels training once you've found your learning rate!\n",
    "        if self.loss < self.best_loss: self.best_loss = self.loss\n",
    "        # see if the loss is better than our best loss - if so, make this better loss our new loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fastai we also use exponential smoothing on the loss. For that reason we check for `best_loss*3` instead of `best_loss*10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_learner(get_model, loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Runner(cb_funcs=[LR_Find, Recorder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.recorder.plot(skip_last=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only does less than 100 epochs, knnow around where we want to stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make it Convolutional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to(train, valid):\n",
    "    m,s = train.mean(), train.std()\n",
    "    return normalize(train, m, s), normalize(valid, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = normalize_to(x_train, x_valid)\n",
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0001), tensor(1.))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When refactoring, use ```Lambda``` layer that takes a basic function input and converts this to a layer that you can add to ```nn.Sequential```.\n",
    "\n",
    "Best to give a name to the function you're using inside of your Lambda (like flatten below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x): return self.func(x)\n",
    "    \n",
    "def flatten(x): return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes flat vector of size bs x 784, puts it back as batch of images in \n",
    "# 28 by 28 pixel resolution\n",
    "def mnist_resize(x): return x.view(-1,1,28,28) # .view resizes stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model(data):\n",
    "    return nn.Sequential(\n",
    "        Lambda(mnist_resize),\n",
    "        nn.Conv2d(1, 8, 5, padding=2, stride=2), nn.ReLU(), #14\n",
    "        nn.Conv2d(8, 16, 3, padding=1, stride=2), nn.ReLU(), #7\n",
    "        nn.Conv2d(16, 32, 3, padding=1, stride =2), nn.ReLU(), #4\n",
    "        nn.Conv2d(32, 32, 3, padding=1, stride =2), nn.ReLU(), #2\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        Lambda(flatten),\n",
    "        nn.Linear(32, data.c)\n",
    "    )\n",
    "\n",
    "# sequential model that contains a bunch of stride 2 convolutions\n",
    "# remember = input is 28 by 28\n",
    "# So after the first convolution, input is 14 (dividing 28 in half)\n",
    "# After the second convolution, input is 7 x 7\n",
    "# then 4 x 4, 2 x 2\n",
    "# Do average pooling of all the activations\n",
    "# flatten the results\n",
    "# pass it through a linear layer\n",
    "# Done!\n",
    "\n",
    "# Use Lambda here to resize our input and flatten our output, all within nn.Sequential itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs = [Recorder, partial(AvgStatsCallback, accuracy)]\n",
    "# partial means I'm treating the AvgStatsCallback class like a function\n",
    "# Here I'm passing it accuracy, and from this I want it to \n",
    "# calculate the average accuracy of my performance per minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr = 0.4) # specifying our optimizer, SGD \n",
    "learn = Learner(model, opt, loss_func, data) # Create our learner, ready for training \n",
    "run = Runner(cb_funcs = cbfs) # Run our model for training, pass in our callback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.4471877734375, tensor(0.8527)]\n",
      "valid: [0.577237451171875, tensor(0.8937)]\n",
      "train: [0.10073755859375, tensor(0.9698)]\n",
      "valid: [0.13612435302734374, tensor(0.9608)]\n",
      "train: [0.072993974609375, tensor(0.9770)]\n",
      "valid: [0.08973518676757812, tensor(0.9712)]\n",
      "train: [0.057587626953125, tensor(0.9825)]\n",
      "valid: [0.06235679321289062, tensor(0.9825)]\n",
      "train: [0.0468539599609375, tensor(0.9853)]\n",
      "valid: [0.16710936279296876, tensor(0.9542)]\n",
      "train: [0.04255091796875, tensor(0.9863)]\n",
      "valid: [0.06998163452148437, tensor(0.9795)]\n",
      "train: [0.03756271728515625, tensor(0.9878)]\n",
      "valid: [0.05567471923828125, tensor(0.9848)]\n",
      "train: [0.0338171533203125, tensor(0.9895)]\n",
      "valid: [0.05486966552734375, tensor(0.9865)]\n",
      "5.87 s  247 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.03292178955078125, tensor(0.9892)]\n",
      "valid: [0.05591175537109375, tensor(0.9860)]\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That took a long time! Let's use our GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple Callback can make sure the model, inputs and targets are all on the same device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the model on the GPU - specifically means the model's parameters, because remember a model only contains two types of numbers, **parameters (or weights) and activations (the things that it's calculating)**. \n",
    "\n",
    "So the **parameters are what are stored on the GPU (not the activations)**.\n",
    "\n",
    "And the inputs to the models and the loss function **also go on the GPU**.\n",
    "\n",
    "Use a CUDA callback:\n",
    "- pass it a device\n",
    "- when you begin fitting, you move the model to that device\n",
    "- when we begin a batch - set the runner's xb and yb and move to the device, which is going to run everything on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda', 0) # most flexible way\n",
    "# allows you to work with multiple GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CudaCallback(Callback):\n",
    "    def __init__(self, device): self.device=device\n",
    "    def begin_fit(self): self.model.to(device)\n",
    "    def begin_batch(self): self.run.xb, self.run.yb = self.xb.to(device), self.yb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(device) # less flexible, but easy\n",
    "# one GPU as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CudaCallback(Callback):\n",
    "    def begin_fit(self): self.model.cuda()\n",
    "    def begin_batch(self): self.run.xb, self.run.yb = self.xb.cuda(), self.yb.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs.append(CudaCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr = 0.4)\n",
    "learn = Learner(model, opt, loss_func, data)\n",
    "run = Runner(cb_funcs = cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.4795509375, tensor(0.8381, device='cuda:0')]\n",
      "valid: [0.10234592895507813, tensor(0.9686, device='cuda:0')]\n",
      "train: [0.09267720703125, tensor(0.9717, device='cuda:0')]\n",
      "valid: [0.07000151977539062, tensor(0.9805, device='cuda:0')]\n",
      "train: [0.06968955078125, tensor(0.9784, device='cuda:0')]\n",
      "valid: [0.06732953491210937, tensor(0.9816, device='cuda:0')]\n",
      "train: [0.0556686376953125, tensor(0.9825, device='cuda:0')]\n",
      "valid: [0.08169276123046874, tensor(0.9784, device='cuda:0')]\n",
      "train: [0.0482647265625, tensor(0.9852, device='cuda:0')]\n",
      "valid: [0.06671342163085937, tensor(0.9832, device='cuda:0')]\n",
      "train: [0.041343720703125, tensor(0.9869, device='cuda:0')]\n",
      "valid: [0.0775447021484375, tensor(0.9813, device='cuda:0')]\n",
      "train: [0.036207958984375, tensor(0.9888, device='cuda:0')]\n",
      "valid: [0.0727143798828125, tensor(0.9800, device='cuda:0')]\n",
      "train: [0.033651142578125, tensor(0.9886, device='cuda:0')]\n",
      "valid: [0.06400704345703125, tensor(0.9840, device='cuda:0')]\n",
      "train: [0.02669377685546875, tensor(0.9916, device='cuda:0')]\n",
      "valid: [0.0698011962890625, tensor(0.9824, device='cuda:0')]\n",
      "train: [0.02758525390625, tensor(0.9912, device='cuda:0')]\n",
      "valid: [0.06576257934570312, tensor(0.9850, device='cuda:0')]\n",
      "train: [0.0250604833984375, tensor(0.9920, device='cuda:0')]\n",
      "valid: [0.492543359375, tensor(0.9112, device='cuda:0')]\n",
      "train: [0.0262292578125, tensor(0.9920, device='cuda:0')]\n",
      "valid: [0.060549725341796874, tensor(0.9871, device='cuda:0')]\n",
      "train: [0.02358283203125, tensor(0.9926, device='cuda:0')]\n",
      "valid: [0.07962757568359374, tensor(0.9849, device='cuda:0')]\n",
      "train: [0.02385116943359375, tensor(0.9917, device='cuda:0')]\n",
      "valid: [0.09092976684570313, tensor(0.9800, device='cuda:0')]\n",
      "train: [0.02254510009765625, tensor(0.9928, device='cuda:0')]\n",
      "valid: [0.071584130859375, tensor(0.9845, device='cuda:0')]\n",
      "train: [0.0184449072265625, tensor(0.9946, device='cuda:0')]\n",
      "valid: [0.06599923095703125, tensor(0.9848, device='cuda:0')]\n",
      "train: [0.019696483154296875, tensor(0.9935, device='cuda:0')]\n",
      "valid: [0.081147802734375, tensor(0.9842, device='cuda:0')]\n",
      "train: [0.02049865478515625, tensor(0.9933, device='cuda:0')]\n",
      "valid: [0.06810540161132812, tensor(0.9858, device='cuda:0')]\n",
      "train: [0.0205787060546875, tensor(0.9934, device='cuda:0')]\n",
      "valid: [0.07086419677734375, tensor(0.9850, device='cuda:0')]\n",
      "train: [0.0231444384765625, tensor(0.9930, device='cuda:0')]\n",
      "valid: [0.07127476806640624, tensor(0.9839, device='cuda:0')]\n",
      "train: [0.015826783447265624, tensor(0.9945, device='cuda:0')]\n",
      "valid: [0.08683189697265625, tensor(0.9864, device='cuda:0')]\n",
      "train: [0.0316392431640625, tensor(0.9906, device='cuda:0')]\n",
      "valid: [0.075757421875, tensor(0.9847, device='cuda:0')]\n",
      "train: [0.019924490966796875, tensor(0.9939, device='cuda:0')]\n",
      "valid: [0.077875390625, tensor(0.9807, device='cuda:0')]\n",
      "train: [0.02253077880859375, tensor(0.9930, device='cuda:0')]\n",
      "valid: [0.103717626953125, tensor(0.9818, device='cuda:0')]\n",
      "7.02 s  309 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.02 seconds for 3 epochs compared to 5.87 seconds for 1 epoch\n",
    "# 2.5 times faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regroup all the conv and relu functions into a single group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(ni, nf, ks=3, stride=2): # using kernel size 3, stride size 2\n",
    "    return nn.Sequential(\n",
    "    nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), nn.ReLU()) # adding a ReLU step here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback transforms the independent variable, x, for a batch\n",
    "# let's you do something like resize images to a specific \n",
    "# tensor size on the fly\n",
    "class BatchTransformXCallback(Callback):\n",
    "    _order = 2\n",
    "    def __init__(self, tfm): self.tfm = tfm\n",
    "    def begin_batch(self): self.run.xb = self.tfm(self.xb)\n",
    "\n",
    "        \n",
    "def view_tfm(*size):\n",
    "    def _inner(x): return x.view(*((-1,)+size))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_view = view_tfm(1, 28, 28) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs.append(partial(BatchTransformXCallback, mnist_view))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With AdaptiveAvgPool - model can now work with any size input\n",
    "\n",
    "nfs = [8,16,32,32] \n",
    "# number of filters I have per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_layers(data, nfs):\n",
    "    nfs = [1] + nfs\n",
    "    return [ # first few layers are for each one of the filters\n",
    "        # conv2d from one filter to the next one\n",
    "        conv2d(nfs[i], nfs[i+1], 5 if i==0 else 3) # kernel size - it's a five for the first layer and 3 for the next layers\n",
    "        for i in range(len(nfs)-1)\n",
    "        \n",
    "# why does the kernel size change?\n",
    "# see markdown below\n",
    "    ] + [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]\n",
    "# last layers - average pooling, flattening, and linear layer\n",
    "def get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data, nfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of filters in the first layer - 8.\n",
    "\n",
    "Image has a single channel, and imagine we were using 3 x 3 filters.\n",
    "At each point in time, looking at 3x3 window with one channel - in total there's 9 input activations it's looking at. And then, spits those into 8 by 9 matrix multiply and then finally into a vector of length 8. Started with 9 numbers, then end up with 8 numbers, so all we're really doing is reordering the numbers, and not doing any useful computation. **So, there's no point in having your first layer just shuffle numbers into a different order. That's why we use a different kernel size for first layer.**\n",
    "\n",
    "\n",
    "ImageNet models - have 27 numbers - 3x3x3 for all three channels. Usually start with a 7 by 7 kernel, not 3x3. For the same reason, **we're making the first layer here have a 5x5 kernel size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy):\n",
    "    if opt_func is None: opt_func = optim.SGD\n",
    "    opt = opt_func(model.parameters(), lr=lr)\n",
    "    learn = Learner(model, opt, loss_func, data)\n",
    "    return learn, Runner(cb_funcs=listify(cbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data, nfs)\n",
    "learn,run = get_runner(model, data, lr=0.4, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (4): AdaptiveAvgPool2d(output_size=1)\n",
       "  (5): Lambda()\n",
       "  (6): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.5143558984375, tensor(0.8247, device='cuda:0')]\n",
      "valid: [0.09866397094726563, tensor(0.9695, device='cuda:0')]\n",
      "train: [0.097219296875, tensor(0.9694, device='cuda:0')]\n",
      "valid: [0.3335225341796875, tensor(0.9157, device='cuda:0')]\n",
      "train: [0.069650908203125, tensor(0.9781, device='cuda:0')]\n",
      "valid: [0.27952236328125, tensor(0.9297, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we make this more accurate, train more stabley, train more quickly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hooks\n",
    "\n",
    "Let's replace PyTorch's ```nn.Sequential``` with our own sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.act_means = [[] for _ in layers]\n",
    "        self.act_stds = [[] for _ in layers]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for i, l in enumerate(self.layers):\n",
    "            x = l(x)\n",
    "            self.act_means[i].append(x.data.mean())\n",
    "            self.act_stds[i].append(x.data.std())\n",
    "        return x\n",
    "    # same as before, except has an act means and act standard deviations compontent in __call__\n",
    "    def __iter__(self): return iter(self.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialModel(*get_cnn_layers(data, nfs)) # nfs = number of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_runner(model, data, lr = 0.9, cbs = cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.942284765625, tensor(0.6757, device='cuda:0')]\n",
      "valid: [0.16282364501953125, tensor(0.9528, device='cuda:0')]\n",
      "train: [0.162400712890625, tensor(0.9527, device='cuda:0')]\n",
      "valid: [0.117707958984375, tensor(0.9668, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
